# 音频

## 基础知识

#### A/D转换与 D/A转换

A/D转换：把音频的模拟信号变成数字信号。

D/A转换：把数字信号转换成模拟信号。

#### 采样率

录音设备在一秒钟内对声音信号的采样次数。

##### 采样定理

奈奎斯特定理

要想重建原始信号，采样频率必须大于信号中最高频率的两倍。一般实际应用中保证采样频率为信号最高频率的2.56～4倍。

##### 频率

人耳能听到的声波的频率范围通常?    20~20000Hz

为了保证声音不失真，采样频率应在40kHz以上。

常用的音频采样频率有：8kHz、11.025kHz、22.05kHz、16kHz、37.8kHz、44.1kHz、48kHz、96kHz、192kHz等。

电话是标准的8khz采样率。

话音信号频率在0.3～3.4kHz范围内，用8kHz的抽样频率，就可获得能取代原来连续话音信号的抽样信号。

而一般CD采集采样频率为44.1kHz。一般情况下的录音都是采用44100Hz的。

###### 窄带和宽带

窄带频率范围为300Hz--3400Hz; 宽带频率范围为50Hz--7000Hz。



#### 量化精度（位宽）

用于表示信号强度。

量化精度：用多少个二进位来表示每一个采样值，也称为量化位数。

声音信号的量化位数一般是 4,6,8,12或16 bits 。一般采用的是16bit。

#### 声道数

由于音频的采集和播放是可以叠加的。

因此，可以同时从多个音频源采集声音，并分别输出到不同的扬声器，故声道数一般表示声音录制时的音源数量或回放时相应的扬声器数量。

单声道（Mono）和双声道（Stereo）比较常见，顾名思义，前者的声道数为1，后者为2。

#### 音频帧

音频数据是流式的，本身没有明确的一帧帧的概念。

在实际的应用中，为了音频算法处理/传输的方便，一般约定俗成取2.5ms~60ms为单位的数据量为一帧音频。

这个时间被称之为“采样时间”，其长度没有特别的标准，它是根据编解码器和具体应用的需求来决定的。

##### 常见音频帧大小

> AAC: 一个AAC帧对应的采样点个数1024, 采样率(samplerate)为 44100Hz
>
> 当前一帧的播放时间 = 1024 * 1000000/44100= 22.32ms(单位为ms)
>
> mp3: 每帧均为1152个字节
>
> 当前一帧的播放时间 = 1152* 1000000/44100= 26.122ms(单位为ms)

#### 码率

码率 = 采样频率 * 采样位数 * 声道个数；

例：采样频率44.1KHz，量化位数16bit，立体声(双声道)，未压缩时的码率 = 44.1KHz * 16 * 2 = 1411.2Kbps = 176.4KBps，即每秒要录制的资源大小。

> 800 bps – 能够分辨的语音所需最低码率（需使用专用的FS-1015语音编解码器）
> 8 kbps —电话质量（使用语音编码）
> 8-500 kbps --Ogg Vorbis和MPEG1 Player1/2/3中使用的有损音频模式
> 500 kbps–1.4 Mbps —44.1KHz的无损音频，解码器为FLAC Audio,WavPack或Monkey's Audio
> 1411.2 - 2822.4 Kbps —脉冲编码调制(PCM)声音格式CD光碟的数字音频
> 5644.8 kbps —SACD使用的Direct Stream Digital格式

#### 名词解析

Audio Source（音频源） 音频的输入端对音频数据进行编码，发送到Sink端。
Audio Sink（音频接收器） 接收到音频数据后，进行解码操作还原出音频。



## 软件架构

#### 语音通信的软件架构

![](.\png\语音通信软件架构.png)

语音发送时由codec芯片采集到语音的模拟信号转换成PCM数据然后通过I2S总线发送给CPU（这是音频驱动负责的，不同的OS会有不同的驱动架构，会在后面音频采集和播放中详细讲，重点讲linux的）。CPU收到PCM数据后首先做前处理，主要包括回音消除(AEC)/噪声抑制(ANS)/自动增益控制(AGC)/VAD等，然后把PCM数据根据指定的codec编码成码流，再打上RTP包头，根据需要还有可能做FEC/重传语音的RTP包等以补偿网络的丢包。最后把生成的RTP包通过UDP socket发到网络中。

语音接收时首先通过UDP socket收RTP包，根据指定看是否需要做FEC等，然后把RTP包或者RTP包的payload(不同的方案有不同的做法)放进jitter buffer中等待取走。从jitter buffer取时取走的是payload，也就是码流。然后对码流解码得到PCM数据，还要看是否需要做PLC/CNG等。最后把PCM数据通过I2S总线送给codec芯片再转换成模拟信号播放出来。这样双方就可以通话了。

#### 播放音乐的软件架构

![](.\png\播放本地音乐的软件架构.png)

首先要解析音频文件头得到相关信息，有codec类型，采样率，通道数等。然后根据这些对每一帧音频数据进行解码得到PCM数据，如果有需要的话还要做后处理，比如均衡器，使音乐更美妙动听。处理完后把PCM数据放进ring buffer中。播放线程会定时的从ring buffer中取走需要的PCM通过I2S总线送给codec芯片，codec芯片把PCM数据转换成模拟信号然后播放出来，这样就可以听到动听的音乐了。

## 采集

A/D转换 ----(I2S/PCM总线)-------CPU

CPU -----(I2S)----- Codec ----------D/A

### Linux

ALSA

### Android

MediaRecoder、AudioRecord

### NDK

OpenSL ES

#### AudioRecord录制 pcm格式音频

##### AudioRecord类介绍

1. AudioRecord构造函数

   ```java
    /**
    * @param audioSource ：录音源
    * @param sampleRateInHz： 采样率
    * @param channelConfig：声道数  
    * @param audioFormat： 采样位数.
    *   AudioFormat#ENCODING_PCM_8BIT,AudioFormat#ENCODING_PCM_16BIT
    *	 AudioFormat#ENCODING_PCM_FLOAT
    * @param bufferSizeInBytes： 音频录制的缓冲区大小
    *   getMinBufferSize(int, int, int)  
    */
   public AudioRecord(int audioSource, int sampleRateInHz, int channelConfig, int audioFormat,
           int bufferSizeInBytes)
   ```

   

2. getMinBufferSize()

   ```java
   /**
   * 获取AudioRecord所需的最小缓冲区大小
   * @param sampleRateInHz： 采样率
   * @param channelConfig：声道数  
   * @param audioFormat： 采样位数.
   */
   public static int getMinBufferSize (int sampleRateInHz, 
               int channelConfig, 
               int audioFormat)
   ```

   

3. getRecordingState()

   ```java
   /**
   * 获取AudioRecord当前的录音状态 
   *   @see AudioRecord#RECORDSTATE_STOPPED    
   *   @see AudioRecord#RECORDSTATE_RECORDING
   */
   public int getRecordingState()
   ```

   

4. startRecording()

   ```java
    /**
    * 开始录制
    */
    public int startRecording()
   ```

   

5. stop()

   ```java
    /**
    * 停止录制
    */
    public int stop()
   ```

   

6. read()

   ```java
   /**
    * 从录音设备中读取音频数据
    * @param audioData 音频数据写入的byte[]缓冲区
    * @param offsetInBytes 偏移量
    * @param sizeInBytes 读取大小
    * @return 返回负数则表示读取失败
    *      see {@link #ERROR_INVALID_OPERATION} -3 ： 初始化错误
           {@link #ERROR_BAD_VALUE}  -3： 参数错误
           {@link #ERROR_DEAD_OBJECT} -6： 
           {@link #ERROR}  
    */
   public int read(@NonNull byte[] audioData, int offsetInBytes, int sizeInBytes) 
   ```


##### 示例

​	代码见  `./demo/采集/Android/RecordHelper.java`

## 音频处理

### 预处理

去噪、静音检测、回声消除

#### 去噪

#### 静音检测

#### 回声消除

`见 《回声消除.md》文档`

### 后处理

音效处理、功放/增强、混音/分离

#### 音效处理

#### 功放/增强

#### 混音/分离

### 开源库



## 编解码与格式转换

> 有损文件格式： 是基于声学心理学的模型，除去人类很难或根本听不到的声音。
>
> 无损格式，例如PCM，WAV，ALS，ALAC，TAK，FLAC，APE，WavPack(WV)
>
> 有损格式，例如MP3，AAC，WMA，Ogg

### 制定压缩标准组织

- ITU，主要制定有线语音的压缩标准（g系列），有g711/g722/g726/g729等。

- 3GPP,主要制定无线语音的压缩标准（amr系列等）, 有amr-nb/amr-wb。后来ITU吸纳了amr-wb，形成了g722.2。
- MPEG,主要制定音乐的压缩标准，有11172-3，13818-3/7，14496-3等。
- 一些大公司或者组织也制定压缩标准，比如iLBC，OPUS。

### 编码格式选择

- 传统通信公司

  - 有线

    ITU-T 的G系列：G711/G722/G726/G729等 ITU-T 的 G系列

  - 无线

    3GPP 的 AMR 系列（AMR-NB/AMR-WB等）

- 移动互联网

  iLBC/opus



### PCM

音频采样过程也叫做脉冲编码调制编码，即PCM（Pulse Code Modulation）编码。

在计算机应用中，能够达到最高保真水平的就是PCM编码。因此，PCM约定俗成了无损编码。

### WAV

音质高 无损格式 体积较大。

WAV对音频流的编码没有硬性规定，除了PCM之外，还有几乎所有支持ACM规范的编码都可以为WAV的音频流进行编码。

### G711

### AMR

压缩比比较大，但相对其他的压缩格式质量比较差，多用于人声，通话录音

> AMR分类:
> AMR(AMR-NB): 语音带宽范围：300－3400Hz，8KHz抽样

### MP3

特点 使用广泛， 有损压缩，牺牲了12KHz到16KHz高音频的音质

### AAC

相对于 mp3，AAC 格式的音质更佳，文件更小，有损压缩，一般苹果或者Android SDK4.1.2（API 16）及以上版本支持播放,性价比高。

### Android

MediaCodec

### 3GPP

3GPP制定的是移动语音的codec标准，主要是AMR（adaptive multi-rate，自适应多码率）系列等，能根据网络状况自适应的调整码率。采样率窄带是8KHz，宽带是16KHz。近年来为了应对互联网的竞争（互联网公司提出了涵盖语音和音乐的OPUS codec），3GPP出台了EVS（enhanced voice service）音频编解码规范。EVS也涵盖了语音和音乐，能在两者之间灵活切换，支持多种采样率和码率。

![](.\png\3GPP.png)

### 开源库



### 编解码优化

#### CPU load优化

##### 优化前的准备工作

1. 通读一下要优化的codec的代码，尽量读懂，即使没懂也要搞清楚函数是干什么的，这有利于后面优化。
2. 准备好profiling工具，profiling工具就是测量运行某个函数花了多少clock。有现成的profiling工具最好，如果没有就根据具体OS和硬件平台（ARM/MIPS等）自己做工具。
3. 准备好test vector，即测试的音源，一般codec制定的官方会提供，通常是多个vector, 对应于不同的场景。优化的原则是在减少CPU load的同时算法运算结果不被改变，所以在做优化时每优化一些就要用test vector跑一下，看结果有没有改变，如果改变了，就要退回到上一个版本。

##### 优化步骤与方法

1. 将编译器的优化选项从-o0改为-o3

2. 给代码中那些经常被调用的又短小的函数加上inline

通常情况下做完1,2后load会下来一大截，如同挤泡沫一样，会挤掉很大一部分。

3. ITU-T或者3GPP的codec reference code中有好多基本运算（加减乘除）的函数，这些函数都写的特别严谨，同时调用的频次又非常高，因而加大了运算复杂度。这些函数中有些在保证正确的前提下可以简化。
4. 用profiling工具一步步排查看到底哪个函数花的load多，明白这个函数是干什么的，然后具体问题具体分析，看怎么样来优化。
5. 有些函数就是一个小算法，reference code中写的比较复杂，调用频次又比较高。要去找有没有简单的实现可以替代，有的话替代了load就会降下来一些。比如codec中经常有求平方根的计算，reference code中通常写的比较复杂。我们知道用牛顿迭代法也可以求平方根，就可以用牛顿迭代法去替换将load降下来。
6. 用汇编优化。如果在C级别能解决问题就不要用汇编了。各个处理器都有自己的汇编指令集，需要去学并且掌握其中的思想和技巧。通常是用的频次较高的又比较占load的函数用汇编去写，即用C和汇编混合编程。汇编优化花的时间会相对长一些。



## 音频传输协议

对于语音来说，实时性要求很高，主要用RTP/UDP做承载，由于UDP是不可靠传输，会丢包乱序等，影响语音质量，所以要采取相应的措施，主要有PLC(丢包补偿)、FEC(前向纠错)、重传、jitter buffer等。

对于音乐来说，以前是播放本地音乐文件，近些年随着网络带宽的加大，可以播放云端的音乐文件了。播放时要把音乐文件传给播放器，一般是边播放边下载，播放音乐对实时性要求不高，一般用HTTP/TCP做承载，也就不存在丢包乱序等问题了。

**实时 RTP/UDP，需要做丢包乱序处理；非实时 HTTP/UDP，不丢包。**



传统通信公司，要严格遵守各种协议（主要有 3GPP/ITU-T/RFC等），不能有自己私有协议，因为要过互通测试。

移动互联网，会用很多私有协议。



### 传输协议

#### RTP + RTCP

![](.\png\RTP-RTCP.png)

RTP来承载语音等实时性要求很高的数据，同时设计了RTCP来保证服务质量（RTP不保证服务质量）。

#### SIP

#### A2DP

#### AVRCP



### 传输处理

#### PLC



#### FEC



#### 重传机制



#### jitter buffer





### 开源库



## 播放



### Android

SoundPool、MediaPlayer、AudioTrack





### 开源库





## 工具

见[音频工具]()



## 参考

[音频开发基础知识简介- davidtym - 博客园](https://www.cnblogs.com/talkaudiodev/p/7041477.html)

[音频开发基础知识- jingchuanhu的专栏- CSDN博客](https://blog.csdn.net/jingchuanhu/article/details/75089110)









[待学习](https://www.cnblogs.com/talkaudiodev/category/1434814.html)

