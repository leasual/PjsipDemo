## 网络质量保障

### 网络环境因素

网络环境因素主要包括延时、乱序、丢包、抖动等，又有多种方法来提高音质，主要有抖动缓冲区（Jitter Buffer）、丢包补偿（PLC）、前向纠错（FEC）、重传等。



#### Jitter Buffer

Jitter Buffer主要针对乱序、抖动因素，主要功能是把乱序的包排好序，同时把包缓存一些时间（几十毫秒）来消除语音包间的抖动使播放的更平滑。[见 jitter buffer.md]



#### FEC

FEC（Forward Error Correction，中文译为前向纠错）主要针对丢包这种因素。它是一种通过增加冗余数据对丢失的数据包进行恢复的信道编码算法。具体地说就是由发送端对原始数据包进行 FEC 编码，生成冗余数据包（原始数据包和冗余数据包的数量比例是固定的）。接收端收到后，通过冗余数据包和原始数据包来恢复出丢失或者出错的数据包。比较成熟的FEC编解码算法有RS(Reeds-Solomon) 算法、Raptor 算法和 Tornado 算法。语音上利用FEC来做补偿主要是在发端对发出的RTP包（几个为一组，称为原始包）FEC编码生成冗余包发给收端，收端收到冗余包后结合原始包用FEC解码得到原始的RTP包从而把丢掉的RTP包补上。至于生成几个冗余包，这取决于接收方反馈过来的丢包率。例如原始包5个为一组，丢包率为30%，经过FEC编码后需要生成两个冗余包，把这7个包都发给对方。对方收到原始包和冗余包的个数和只要达到5个就可以通过FEC完美复原出5个原始包，这5个原始包中丢掉的就通过这种方式补偿出来了。下图展示了这个过程，发送端有5个原始RTP包，通过FEC编码生成2个冗余RTP包。由于生成两个冗余包， 算法最多能够恢复 2 个丢失的原始包。发送端把原始包和冗余包发出去，在传输过程中假设第 2 号和4号原始包丢失了。接收端接收到原始包和冗余包后，通过原始包1、3、5和 冗余包1 、2就可以把已经丢失的第 2 号和4号原始数据包恢复出来。

![](.\png\FEC%E8%BF%87%E7%A8%8B.png)

原始RTP包有包头和payload，冗余包中还要加上一个FEC头（在RTP头和payload中间），FEC头结构如下：

![](.\png\FEC%E5%A4%B4.png)

Group first Sequence number是指这一组原始包中第一个的sequence number

original count是指一组原始包的个数

redundant count是指生成的冗余包的个数

Redundant index是指第几个冗余包

冗余包有自己的payload type 和sequence number，要在SIP的SDP中告诉对方冗余包的payload type是多少，对方收到这个payload type的包后就做冗余包处理。



FEC不依赖与语音包内的payload，对于丢失的包能精确的复原出来。但是它也有缺点，一是它要累积到指定数量的包才能精确的复原，这就增加了时延；二是它要产生冗余包发送给对方，增加了流量。



#### PLC

PLC也主要针对丢包因素。它本质上是一种信号处理方法，利用前面收到的一个或者几个包来近似的产生出当前丢的包。产生补偿包的技术有很多种，比如基音波形复制（G711 Appendix A PLC用的就是这种技术）、波形相似叠加技术（WSOLA）、基音同步叠加（PSOLA）技术等。

对codec而言，如果支持PLC功能，如G729，就不需要再在外部加PLC功能了，只需要对codec做相应的配置，让它的PLC功能使能。如果不支持PLC功能，如G711，就需要在外部实现PLC。

PLC对小的丢包率（< 15%）有比较好的效果，大的丢包率效果就不好了，尤其是连续丢包，第一个丢的包补偿效果还不错，越到后面丢的包效果越差。



#### 总结

把Jitter Buffer、FEC、PLC结合起来就可以得到如下的接收侧针对网络环境因素的提高音质方案：

![](.\png\%E9%9F%B3%E9%A2%91%E7%BD%91%E7%BB%9C%E5%A4%84%E7%90%86.png)

从网络收到的RTP包如是原始包不仅要PUT进JB，还要PUT进FEC。如是冗余包则只PUT进FEC，在FEC中如果一组包中原始包的个数加上冗余包的个数达到指定值就开始做FEC解码得到丢失的原始包，并把那些丢失的原始包PUT进JB。在需要的时候把语音帧从JB中GET出解码并有可能做PLC。



#### 重传（NACK）

重传也主要针对丢包这种因素，把丢掉的包再重新传给对方，一般都是采用按需重传的方法。我在用重传的方法时是这样做的：接收方把收到的包排好序后放在buffer里，如果收到RTP包头中的sequence number能被5整除（即模5），就统计一下这个包前面未被播放的包有哪些没收到（即buffer里相应位置为空）， 采用比特位的方式记录下来（当前能被5整除的包的前一个包用比特位0表示，丢包置1，不丢包置0，比特位共16位（short型），所以做多可以看到前16个包是否有丢包），然后组成一个控制包（控制包的payload有两方面信息：当前能被5整除的包的sequence number（short型）以及上面组成的16位的比特位）发给对方，让对方重发这些包。接收方收到这个控制包后就能解析出哪些包丢了，然后重传这些包。在控制包的payload里面也可以把每个丢了的包的sequence number发给对方，这里用比特位主要是减小payload大小，省流量。

 

在实际使用中重传起的效果不大，主要是因为经常重传包来的太迟，已经错过了播放窗口而只能主动丢弃了。它是这些方法中效果最差的一个。



#### PFC2198

RFC2198是RTP Payload for Redundant Audio Data（用于冗余音频数据的RTP负载格式），用了它后在当前RTP包中不仅可以承载当前语音的payload，还可以承载前几个包的payload，**承载以前包**的个数越多，在高丢包率的情况下效果越好，但是延时也就越大，同时消耗的流量也就越多。相比于FEC，它消耗的流量更多，因为FEC用一组RTP包编码生一个或多个成冗余包，而它一个RTP包就带一个或多个以前包的payload。在有线网络或者WIFI下可以用，在蜂窝网络下建议慎用。



### webrtc jitter buffer

webrtc的jitterbuffer相当优秀，按照功能分类的话，可以分为jitter和buffer。buffer主要对丢包、乱序、延时到达等异常情况做处理，还会和NACK、FEC、FIR等QOS相互配合。jitter主要根据当前帧的大小和延时评估出jitter delay，再结合decode delay、render delay以及音视频同步延时，得到render time，来控制平稳的渲染视频帧。

![](.\png\webrtc jitter buffer运行机制.png)

#### buffer对接收到的rtp包的处理流程如下：

1. 第一次接收到一个视频包，从freeframes队列中弹出一个空frame块，用来放置这个包。
2. 之后每次接收到一个RTP包，根据时间戳在incompleteframes和decodableframes中寻找，看是否已经接收到过相同时间戳的包，如果找到，则弹出该frame块，否则，从freeframes弹出一个空frame。
3. 根据包的序列号，找到应该插入frame的位置，并更新state。其中state有empty、incomplete、decodable和complete，empty为没有数据的状态，incomplete为至少有一个包的状态，decodable为可解码状态，complete为这一帧所有数据都已经到齐。decodable会根据decode_error_mode 有不同的规则，QOS的不同策略会设置不同的decode_error_mode ，包含kNoErrors、kSelectiveErrors以及kWithErrors。decode_error_mode 就决定了解码线程从buffer中取出来的帧是否包含错误，即当前帧是否有丢包。
4. 根据不同的state将frame帧 push回到队列中去。其中state为incomplete时，push到incompleteframes队列，decodable和complete状态的frame，push回到decodableframes队列中。
5. freeframes队列有初始size，freeframes队列为空时，会增加队列size，但有最大值。也会定期从incompleteframes和decodable队列中清除一些过时的frame，push到freeframes队列。
6. 解码线程取出frame,解码完成之后，push回freeframes队列。

jitterbuffer与QOS策略联系紧密，比如，incompleteframes和decodable队列清除一些frame之后，需要FIR（关键帧请求），根据包序号检测到丢包之后要NACK（丢包重传）等。



#### jitter

所谓jitter就是一种抖动。具体如何解释呢？从源地址发送到目标地址，会发生不一样的延迟，这样的延迟变动就是jitter。
jitter会带来什么影响？jitter会让音视频的播放不平稳，如音频的颤音，视频的忽快忽慢。那么如何对抗jitter呢？增加延时。需要增加一个因为jitter而存在的delay，即jitterdelay。

![](.\png\webrtc jitter delay.png)

其中，frameDelayMS指的是一帧数据因为分包和网络传输所造成的延时总和,帧间延迟。具体如下图，即RTP1和RTP2到达Receiver的时间差。

![](.\png\webrtc jitter frameDelay.png)

framesizeBytes指当前帧数据大小，incompleteFrame指是否为完整的帧，UpdateEstimate为根据这三个参数来更新jitterdelay的模块，这个模块为核心模块，其中会用到卡尔曼滤波对帧间延迟进行滤波。

```c
JitterDelay = theta[0] * (MaxFS – AvgFS) + [noiseStdDevs * sqrt(varNoise) – noiseStdDevOffset]
```

其中theta[0]是信道传输速率的倒数，MaxFS是自会话开始以来所收到的最大帧大小，AvgFS表示平均帧大小。noiseStdDevs表示噪声系数2.33，varNoise表示噪声方差，noiseStdDevOffset是噪声扣除常数30。UpdateEstimate会不断地对varNoise等进行更新。

在得到jitterdelay之后，通过jitterdelay+ decodedelay + renderdelay，再确保大于音视频同步的延时，加上当前系统时间得到rendertime，这样就可以控制播放时间。控制播放，也就间接控制了buffer的大小。

