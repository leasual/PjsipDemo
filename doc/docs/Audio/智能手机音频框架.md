## Android 音频框架

### 音频框架图

![](.\png\Android智能手机音频相关硬件框架图.png)

AP是应用处理器（application processor），现在用的最多的是ARM的处理器，在上面主要运行的是操作系统（OS，例如android）和应用程序。

CP是通信处理器（communication processor），也叫基带处理器（baseband processor，BP）或者modem，上面主要处理跟通信相关的。

蓝牙耳机比较特殊，它是直接通过I2S总线与audio DSP连接的，主要是因为音频的采集和播放在蓝牙芯片里都有了。当用蓝牙耳机听音乐时，音频码流在AP上解码成PCM数据用A2DP协议经过UART直接送给蓝牙耳机播放，而不是经过audio DSP通过IIS总线送给蓝牙耳机播放。



#### AP

Android里有multimedia framework，audio是其中的一部分，AP上audio部分的软件框图如下：

![](.\png\Android智能手机音频框架-AP.png)

#### DSP

![](.\png\Android智能手机音频框架-DSP.png)

模块主要有codec（MP3/AAC/AMR/EVS等）、前处理（AGC/ANS/AGC等）、后处理（EQ/AGC/ANS 等）、重采样（SRC）、混音（MIX）、从DMA获取采集到的音频数据（CAPTURE）、将音频数据送给DMA后播放（PLAY）等，当然还有接收和发送给其他处理器的音频数据处理等，AP和CP都要与audio DSP交互语音数据。

#### CP

4G下的音频软件处理框图：

![](.\png\Android智能手机音频框架-CP.png)

主要分两大部分，IMS stub，处理IMS（IP Multimedia Subsystem, IP多媒体子系统）中的语音数据相关的（IMS 控制协议相关的在AP中处理）。Audio，对IMS中语音数据的pack/unpack以及与audio DSP的交互等。



### 音频数据流向

手机中音频的主要场景有音频播放、音频录制、语音通信等。不同场景下的音频数据流向有很大差异，即使是同一场景，在不同的模式下音频数据流向也有所不同。

#### 音频播放

Android系统audio框架中主要有三种播放模式：low latency playback、deep buffer playback和compressed offload playback。

##### low latency playback

用于按键音、游戏背景音等对时延要求高的声音输出。音频文件是在AP侧解码成PCM数据，然后再经由Audio DSP送给codec芯片播放出来。

##### deep buffer playback

用于音乐等对时延要求不高的声音输出。音频文件是在AP侧解码成PCM数据，如果有音效的话会再对PCM数据处理（android audio framework中有effect音效模块，支持的音效有均衡器、低音增强、环绕声等），然后再经由Audio DSP送给codec芯片播放出来。

##### compressed offload playback

用于音乐等声音输出，但是音频解码部分的工作是在Audio DSP中完成，AP侧只负责把音频码流送到Audo DSP中，送出去后AP侧会进行休眠，Audo DSP中会分配一块较大的buffer去处理此数据，在Audo DSP中进行解码、音效的处理等工作，在Audo DSP解码器处理完数据之前，它会唤醒AP侧去送下一包数据。用这种模式播放音频能有效的降低功耗，是最为推荐的播放音乐的模式。但是在目前的主流的音乐播放APP中用的基本上都是deep buffer的播放模式，比如QQ音乐、网易云音乐和酷狗音乐等。



至于哪些格式的音乐用这种模式播放，这需要在audioPolicy中去控制。



播放系统音和游戏音用low latency 模式*，播放*音乐用deep buffer或者compressed offload模式，播放录音用deep buffer模式。



##### low latency / deep buffer模式下的音频数据流向

![](.\png\Android音频播放-low latency or deep buffer.png)

音频文件先在AP上软解码得到PCM后经过AudioTrack/audioFlinger中的Mixer thread(有可能要做音效后处理)/audio HAL/tinyALSA后送给kernel，然后用IPC将PCM送给Audio DSP经重采样混音等后播放出来。由于在AP上已做解码和音效后处理，Audio DSP上就不需要做了。

##### compressed offload模式下的音频数据流向

![](.\png\Android音频播放 compressed offload模式音频流向.png)

音频码流经过AP上的AudioTrack/audioFlinger中的Offload thread(不需要做音效后处理)/audio HAL/tinyALSA后送给kernel，然后用IPC将码流送给Audio DSP经解码、后处理、重采样、混音等后播放出来。



#### 音频录制

下图就是录音时音频数据的流向。同音频播放一样，录音时音频数据也是只经过AP和audio DSP。

![](.\png\Android音频录制.png)

codec芯片采集到的PCM数据送给Audio DSP经重采样、前处理后送给AP的kernel，再经由tinyALSA/audio HAL /audioFlinger中的Record thread/audioRecord等后做软编码得到录音码流文件并保持起来。

#### 语音通信

语音通信就是打电话啦。它同音频播放或者录制不一样，它是双向的，分上行（uplink,把采集到的语音发送给对方）和下行(downlink，把收到的语音播放出来)，而音频播放或者录制是单向的。它音频数据流向也跟音频播放或者录制不一样，只经过audio DSP和CP，下图就是打电话时音频数据的流向。

![](.\png\Android语音通信音频数据流.png)

在上行方向上codec芯片采集到的PCM数据送给Audio DSP经重采样、前处理(AEC/ANS/AGC等)、编码后得到码流，并将码流送给CP，CP处理后经过空口(air interface)送给对方。在下行方向上先从空口收对方送过来的语音数据，并做网络侧处理(jitter buffer等)，然后送给Audio DSP，Audio DSP收到后做解码、后处理(ANS/AGC等)、重采样等，再把PCM数据经DMA/I2S送给codec芯片播放出来。



### Android手机上开发的APP上的语音解决方案

![](.\png\Android app开发音频框架图.png)

本方案是在AP（应用处理器）上实现，语音采集和播放并没有直接调用系统提供的API（AudioTrack/AudioRecorder）, 而是用了开源库openSL ES，让openSL ES去调用系统的API。我们会向openSL ES注册两个callback函数（一个用于采集语音，一个用于播放语音），这两个callback每隔20Ms被调用一次，分别获得采集到的语音以及把收到的语音送给底层播放，从底层拿到的以及送给底层的语音数据都是PCM格式，都配置成16k采样率单声道的模式。在上行方向，codec芯片采集到的语音PCM数据通过I2S送给audio DSP，audio DSP处理后把PCM数据送给AP，最终通过注册的采集callback函数把PCM数据送给上层。在上层先做前处理，包括AEC/AGC/ANS等，用的是webRTC的实现（现在APP语音内的前后处理基本上用的都是webRTC的实现），做完前处理后还要根据codec看是否需要做重采样，如codec是8k采样率的，就需要做重采样（16k转到8k）, 如codec是16k采样率的，就不需要做重采样了。再之后是编码得到码流，同时用RTP打包，并用UDP socket把RTP包发给对方。在下行方向，先用UDP socket收语音RTP包，去包头得到码流放进jitter buffer中。每隔20Ms会从jitter buffer中拿一帧数据解码得到PCM，有可能还要做PLC和重采样，然后再做后处理（ANS/AGC）,最终通过播放callback函数把PCM数据一层层往下送给Audio DSP，audio DSP处理后把PCM数据送给codec芯片播放出来。APP上的语音通信属于OTT （On The Top）语音，不像传统语音通信那样有QoS保障，要保证语音质量，必须要采取更多的补偿措施（由于无线网络环境是变化多端的，经常会比较恶劣，会导致乱序丢包等），常见的方法有FEC（前向纠错）、重传、PLC（丢包补偿）等。补偿措施是该方案的难点，通过补偿把语音质量提高了，但是增大了时延和增加了流量。



### Android手机上Audio DSP频率低 memory小的应对措施



#### 频率低

1. 浮点运算定点话来节约时间，即用定点数表示浮点数。定点数中最高位表示符号位，符号位右边n位表示整数，剩下的表示小数。这种表示方法叫Q格式。
2. 选择算法实现时一定要用定点实现。

#### memory低的应对措施

DSP的内部memory分两种：

- DTCM（Data Tightly Coupled Memory, 数据紧密耦合存储器）

  DTCM用于存data

- PTCM（Program Tightly Coupled Memory, 程序紧密耦合存储器）。

  PTCM用于存code。

同时还有外部memory（DDR），它也可存data和code。data和code尽量放内部，因为这样速度快效率高，不到万不得已才将它们放在外部memory上。

##### DTCM

DTCM主要分以下几个区域：const区、data区、bss区、overlay区。const区顾名思义就是放一些常量的，data区是放已初始化的全局变量和静态变量，bss区是放未初始化的全局变量和静态变量，**overlay区主要是根据场景的互斥做一些memory的复用，这是应对memory小的主要措施**。我们先看overlay机制。

![](.\png\DSP memory DTCM overlay 区.png)

上图中memory地址是由低向高增长，分别是const区、data区、bss区、overlay区。在overlay区music和voice有相同的起始地址，在music中decoder MP3和AAC又有相同的起始地址，在voice中codec AMR-NB、AMR-WB和EVS又有相同的起始地址。



##### 其他

1. 定义数据类型时能用short的就不要用int
2. 在overlay区域，buffer的大小都是指定的。指定时要正确算出大小值，不要指定大了，指定大了就浪费了。
3. 在data区或者bss区的buffer要看是不是分大了，比如有的buffer分三块就够了，也就没必要分四块了，分四块一是浪费了buffer，二是有些场景下增加了时延。
4.  DSP上每个thread/task的栈的大小都是指定的。为了省memory，栈的大小不可能很大，一般不超过1k word。这就要求写代码时不能有大的局部变量数组等，遇到时就要通过一些技巧解决。
5. 代码编好后会生成一个各buffer起始地址和大小的文件。关注那些size较大的buffer，分析有没有减小的可能。

